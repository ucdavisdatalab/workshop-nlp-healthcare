[["index.html", "NLP for Healthcare 1 Overview 1.1 Objectives for this Workshop 1.2 Demo introduction", " NLP for Healthcare Dr. Pamela Reynolds Dr. Wesley Brooks Arthur Koehl 2021-02-26 1 Overview link to lesson plan: https://ucdavisdatalab.github.io/workshop-nlp-healthcare/ link to github repo: https://github.com/ucdavisdatalab/workshop-nlp-healthcare link to data used: abstracts.csv If you will be coding along please check that you have R and RStudio installed on your machine. 1.1 Objectives for this Workshop Explain natural language processing in lay terms Give examples of NLP applications for research Evaluate particular challenges posed by working with health data Describe key text mining and NLP metrics for assessing word importance and document similarity Install the ‘tm’ package in R Create a corpus and DocumentTermMatrix Calculate word frequencies Calculate TF-IDF weighting Generate PCA for exploring document similarities identify where to go to learn more! 1.2 Demo introduction Dr. Sonja Diertrich is a radiation oncologist at UCDMC studying early stage breast cancer. For this coding demo we are using NLP to explore the recent literature on this topic with her lab. 1.2.1 About the data Working with Blaisdell Medical Library’s health librarian Amy Studer, we conducted a pilot systematic search in EMBASE and PubMed on this topic for articles published in the International Journal of Radiation Oncology Biology Physics, Practical Radiation Oncology, and Lancet over the past 5 years. For this demo we are using the titles and abstracts from that search. (See the readME for more information regarding the search strategy used to generate our datatset.) 1.2.2 Why R? What is RStudio? “R” is a programming language and statistical software that interprets scripts. RStudio is an Integrated Development Environment (IDE) for working with R. You need R installed in order to use RStudio, but you don’t need RStudio to work with R. So why R? Faciliates reproducible research workflows - integrates with other tools. - designed for data analysis - produces high-quality graphics It’s extensible and interdisciplinary - free, open-source, well-documented, and cross-platform - runs on Mac, PC and Linux. - works with data of all shapes and sizes. - large library of external packages available for performing diverse tasks, including text mining and NLP. - Large (and growing) community Should you use R? Maybe. Use whatever langauge allows you to perform your work and faciliates sharing with your direct colleagues and the broader reserach community. "],["intro-to-text-processing-and-nlp-for-health-data.html", "2 Intro to Text Processing and NLP for Health Data 2.1 General Methods for Analyzing Text 2.2 NLP 2.3 What’s a Natural Language? 2.4 NLP Trends 2.5 Caveats before we get started 2.6 Further resources", " 2 Intro to Text Processing and NLP for Health Data There is a lot of health data that is unstructured text. EMR Clinical notes Medical journal literature Surveys and questionnaires Interviews Community forums Social media The goal is to take text data -&gt; convert it into data that can be analyzed (numbers) -&gt; run some analysis -&gt; interpret the analysis. 2.1 General Methods for Analyzing Text word frequency TF-IDF PCA tokenization KWIC: Key Word in Context co-occurence stemming lemmatization - removing inflectional endings to return the base dictionary form of a word (lemma). bigrams: conditional probability of a token given the preceding token. named entity recognition (NER): identifying which items in the text map to proper names (people, places, location). regular expressions (RegEx): searching for exact words, parts of words or phrases feature based linear classifiers topic modeling word embedding: mapping words or phrases to vectors of real numbers allowing words with similar meaning to have a similar representation. sentiment analysis: subjective information to determine “polarity” of a document. (e.g., positive or negative reviews) part of speech tagging: determining the parts of speech for each word in a sentence. 2.2 NLP NLP allows us to use “distance reading” to unlock information from narrative text for extraction and classification: keyword detection topic detection document summarizing document classification document clustering document similarity speech recognition text translation Natural Language Processing (NLP) = linguistics + computer science + information engineering + data science. We use NLP to computationally parse and analyze large amounts of natural language data. 2.3 What’s a Natural Language? “…any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.” Thanks, Wikipedia. Thought questions: Are clinical notes a natural language? What about tweets? 2.4 NLP Trends 1950s - 1970s Using our understanding of language to develop rules that we feed into computer programs. 1980s - 2000s Classical NLP - Using statistical methods to analyze text corpora. 2010s - Further extension of statistical analysis of corpora and developments in deep learning. More and more data available, untagged. Need for unsupervised methods. In this workshop we’re focusing on classical NLP (no deep learning). 2.5 Caveats before we get started Garbage in, garbage out. Know your corpus. All models are wrong, but some are useful. And others are dangerous. Natural langauge data can not be deidentified. NLP models are powerful, but can fail when applied to jargon-heavy, niched domains. Augmenting with developed medical dictionaries helps, but parts of speech taggers, for example, are not designed for the health space. This is especially true for classical NLP techniques, although deep learning approaches hold strong promise for working with unstructured health data. 2.6 Further resources DataLab’s NLP Researcher Toolkit Curated list of ML and NLP resources for healthcare Post on how NLP can help clinicians Useful packages: ntlk (python) spaCy (python) quanteda (R) tidytext (R) Classes: Dan Jurafsky and Christopher Manning NLP youtube series CS 124 Slides on Text Processing Datasets: https://github.com/niderhoff/nlp-datasets https://archive.ics.uci.edu/ml/datasets.php?format=&amp;task=&amp;att=&amp;area=&amp;numAtt=&amp;numIns=&amp;type=text&amp;sort=nameUp&amp;view=table "],["working-with-text-data-in-r.html", "3 Working with Text Data in R 3.1 Setup 3.2 Preprocessing 3.3 The Bag of Words Representation 3.4 The Document Term Matrix 3.5 TF-IDF", " 3 Working with Text Data in R 3.1 Setup 3.1.1 Packages In R, and most programming languages, there are many packages - code written by other people to help with certain tasks. For this workshop we will be using two packages - ‘tm’ and ‘ggplot2’. The ‘tm’ - text mining - package has methods for mining text with R including importing data, storing corpora, applying operations on corpora (such as common preprocessing methods), and document term matrices. The ‘ggplot2’ package has functions related to plotting and visualizing data. Run this command if you don’t already have these packages installed. install.packages(&#39;tm&#39;) install.packages(&#39;ggplot2&#39;) install.packages(&#39;Matrix&#39;) install.packages(&#39;readr&#39;) Once the packages are installed, load them into your R environment. library(&#39;tm&#39;) library(&#39;ggplot2&#39;) library(&#39;Matrix&#39;) library(&#39;readr&#39;) You can find the documentation for the package online Within R/RStudio you can browse function documentation with the following syntax. ?TermDocumentMatrix 3.1.2 Data for this workshop For this workshop, we are looking at a set of abstracts of medical journal articles related to breast cancer. We have 714 abstracts, stored in a csv, with duplicates. We would like to get the key words from each abstract, as well as visualize / check for groupings of abstracts in two dimensions. data &lt;- read_csv(url(&quot;https://ucdavisdatalab.github.io/workshop-nlp-healthcare/abstracts.csv&quot;)) ## ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## cols( ## authors = col_character(), ## year = col_double(), ## title = col_character(), ## journal = col_character(), ## text = col_character() ## ) head(data) ## # A tibble: 6 x 5 ## authors year title journal text ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; 2008 The UK Standardisatio… The Lancet Background: The inte… ## 2 &lt;NA&gt; 2014 Erratum: Effect of ra… The Lancet EBCTCG (Early Breast… ## 3 &lt;NA&gt; 2015 Proceedings of the Am… International Jour… The proceedings cont… ## 4 &lt;NA&gt; 2016 Late-Breaking Abstrac… International Jour… The proceedings cont… ## 5 &lt;NA&gt; 2016 Proceedings of the Am… International Jour… The proceedings cont… ## 6 &lt;NA&gt; 2017 2017 ASTRO Annual Mee… International Jour… The proceedings cont… We have a dataframe with 714 rows, each row referring to a different abstract. For each abstract we have the authors, year published, title of the paper, name of the journal, and the full text. Let’s look at the text from the first abstract data$text[[1]] ## [1] &quot;Background: The international standard radiotherapy schedule for early breast cancer delivers 50 Gy in 25 fractions of 2·0 Gy over 5 weeks, but there is a long history of non-standard regimens delivering a lower total dose using fewer, larger fractions (hypofractionation). We aimed to test the benefits of radiotherapy schedules using fraction sizes larger than 2·0 Gy in terms of local-regional tumour control, normal tissue responses, quality of life, and economic consequences in women prescribed post-operative radiotherapy. Methods: Between 1999 and 2001, 2215 women with early breast cancer (pT1-3a pN0-1 M0) at 23 centres in the UK were randomly assigned after primary surgery to receive 50 Gy in 25 fractions of 2·0 Gy over 5 weeks or 40 Gy in 15 fractions of 2·67 Gy over 3 weeks. Women were eligible for the trial if they were aged over 18 years, did not have an immediate reconstruction, and were available for follow-up. Randomisation method was computer generated and was not blinded. The protocol-specified principal endpoints were local-regional tumour relapse, defined as reappearance of cancer at irradiated sites, late normal tissue effects, and quality of life. Analysis was by intention to treat. This study is registered as an International Standard Randomised Controlled Trial, number ISRCTN59368779. Findings: 1105 women were assigned to the 50 Gy group and 1110 to the 40 Gy group. After a median follow up of 6·0 years (IQR 5·0-6·2) the rate of local-regional tumour relapse at 5 years was 2·2% (95% CI 1·3-3·1) in the 40 Gy group and 3·3% (95% CI 2·2 to 4·5) in the 50 Gy group, representing an absolute difference of -0·7% (95% CI -1·7% to 0·9%)-ie, the absolute difference in local-regional relapse could be up to 1·7% better and at most 1% worse after 40 Gy than after 50 Gy. Photographic and patient self-assessments indicated lower rates of late adverse effects after 40 Gy than after 50 Gy. Interpretation: A radiation schedule delivering 40 Gy in 15 fractions seems to offer rates of local-regional tumour relapse and late adverse effects at least as favourable as the standard schedule of 50 Gy in 25 fractions.&quot; Notice that within the text there are a variety of potential issues. For example, some words are capitalized, there is punctuation, weird symbols, and numbers. For many NLP methods, we want to normalize the texts to get around these issues. The ‘tm’ package has several built in features for normalizing text. The first step is to load the text into a ‘corpus’ object. 3.2 Preprocessing 3.2.1 Load the text column into a ‘corpus’ object mycorpus &lt;- Corpus(VectorSource(data$text)) inspect(head(mycorpus)) ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 6 ## ## [1] Background: The international standard radiotherapy schedule for early breast cancer delivers 50 Gy in 25 fractions of 2·0 Gy over 5 weeks, but there is a long history of non-standard regimens delivering a lower total dose using fewer, larger fractions (hypofractionation). We aimed to test the benefits of radiotherapy schedules using fraction sizes larger than 2·0 Gy in terms of local-regional tumour control, normal tissue responses, quality of life, and economic consequences in women prescribed post-operative radiotherapy. Methods: Between 1999 and 2001, 2215 women with early breast cancer (pT1-3a pN0-1 M0) at 23 centres in the UK were randomly assigned after primary surgery to receive 50 Gy in 25 fractions of 2·0 Gy over 5 weeks or 40 Gy in 15 fractions of 2·67 Gy over 3 weeks. Women were eligible for the trial if they were aged over 18 years, did not have an immediate reconstruction, and were available for follow-up. Randomisation method was computer generated and was not blinded. The protocol-specified principal endpoints were local-regional tumour relapse, defined as reappearance of cancer at irradiated sites, late normal tissue effects, and quality of life. Analysis was by intention to treat. This study is registered as an International Standard Randomised Controlled Trial, number ISRCTN59368779. Findings: 1105 women were assigned to the 50 Gy group and 1110 to the 40 Gy group. After a median follow up of 6·0 years (IQR 5·0-6·2) the rate of local-regional tumour relapse at 5 years was 2·2% (95% CI 1·3-3·1) in the 40 Gy group and 3·3% (95% CI 2·2 to 4·5) in the 50 Gy group, representing an absolute difference of -0·7% (95% CI -1·7% to 0·9%)-ie, the absolute difference in local-regional relapse could be up to 1·7% better and at most 1% worse after 40 Gy than after 50 Gy. Photographic and patient self-assessments indicated lower rates of late adverse effects after 40 Gy than after 50 Gy. Interpretation: A radiation schedule delivering 40 Gy in 15 fractions seems to offer rates of local-regional tumour relapse and late adverse effects at least as favourable as the standard schedule of 50 Gy in 25 fractions. ## [2] EBCTCG (Early Breast Cancer Trialists&#39; Collaborative Group). Effect of radiotherapy after mastectomy and axillary surgery on 10-year recurrence and 20-year breast cancer mortality: meta-analysis of individual patient data for 8135 women in 22 randomised trials. Lancet 2014; 383: 2127–35—During revision of the appendix of this Article after peer review, some analyses of local recurrence were omitted. These have now been restored in the updated appendix. All analyses (and page numbers) in the previous version of the appendix remain unchanged, as do the medical findings of the paper. The correction has been made to the appendix as of Nov 21, 2014. ## [3] The proceedings contain 2121 papers. The topics discussed include: radiation treatment parameters and late gastrointestinal toxicity in cervical cancer patients treated with image guided high-dose-rate brachytherapy; the effect of pelvic radiation therapy on vaginal brachytherapy cylinder size; role of radiation therapy in platinum-resistant recurrent ovarian cancer diagnosed by FDGPET/contrast-enhanced CT; inferior outcomes following definitive radiation therapy or concurrent chemoradiation therapy for adenocarcinoma than squamous cell carcinoma of uterine cervix: a matched case control study; first safety analysis after 80 treated patients with early breast cancer within the targit-e trial; accelerated partial-breast irradiation (APBI) provides equivalent 5 year and 10 year outcomes regardless of molecular subtype; and breast cancer patients&#39; preferences for adjuvant radiation therapy post lumpectomy, whole-breast irradiation versus partial-breast irradiation: a single-institutional study. ## [4] The proceedings contain 12 papers. The topics discussed include: dexamethasone versus placebo in the prophylaxis of radiation-induced pain flare following palliative radiation therapy for bone metastases: a double-blind randomized, controlled, superiority trial; hypofractionated versus conventionally fractionated radiation therapy for prostate cancer: five-year oncologic outcomes of the Dutch randomized phase 3 HYPRO trial; patient-reported outcomes in NRG oncology/RTOG 0938, a randomized phase 2 study evaluating 2 ultrahypofractionated regimens (UHRs) for prostate cancer; report of NRG oncology/RTOG 9601, a phase 3 trial in prostate cancer: anti-androgen therapy (AAT) with bicalutamide during and after radiation therapy (RT) in patients following radical prostatectomy (RP) with pT2-3pN0 disease and an elevated PSA; NRG oncology RTOG 0415: a randomized phase 3 noninferiority study comparing 2 fractionation schedules in patients with low-risk prostate cancer; accelerated partial breast irradiation using sole interstitial multicatheter brachytherapy versus whole breast irradiation for early breast cancer: five-year results of a randomized phase 3 trial - part I: local control and survival results; preclinical advances in combined-modality cancer immunotherapy with radiation therapy; and examination of industry payments to radiation oncologists in 2014 using the centers for medicare and medicaid services open payments database. ## [5] The proceedings contain 2300 papers. The topics discussed include: a phase III randomized control trial comparing skin-sparing helical tomotherapy versus 3D-conformal radiation therapy in early-stage breast cancer: acute and late skin toxicity outcomes; longitudinal analysis of patient-reported outcomes and cosmesis in a randomized trial of conventionally fractionated versus hypofractionated whole-breast irradiation; brentuximab vedotin and AVD chemotherapy followed by ISRT: a safe primary treatment regimen for early-stage, unfavorable Hodgkin lymphoma; treatment of early-stage unfavorable Hodgkin lymphoma: efficacy and toxicity of 4 versus 6 cycles of ABVD chemotherapy with radiation; and a prospective pilot study evaluating feasibility and utility of ECG-gated CT angiography for coronary-sparing radiation therapy planning in mediastinal lymphoma. ## [6] The proceedings contain 16 papers. The topics discussed include: tumor treating fields (TTFields)- a novel cancer treatment modality: translating preclinical evidence and engineering into a survival benefit with delayed decline in quality of life; a randomized controlled trial evaluating the utility of a patient decision aid to improve clinical trial (RAVES 08.03) related decision-making; healthcare disparities in cancer patients receiving radiation: changes in insurance status after medicaid expansion under the affordable care act; two-year results for MC1273, a phase 2 evaluation of aggressive dose de-escalation for adjuvant chemoradiation in HPV+ oropharynx squamous cell carcinoma (OPSCC); PACIFIC: a double-blind, placebo-controlled phase 3 study of durvalumab as consolidation therapy after chemoradiation in patients with locally advanced, unresectable nonesmall cell lung cancer; 68Ga-PSMA PET/CT mapping of early biochemical recurrence (PSA1 ng/mL) after primary surgery in 270 patients: impact on salvage radiation therapy planning; prospective validation of transforming growth factor-beta (TGF-B) polymorphism C509T as a predictor of radiation-induced fibrosis in early stage breast cancer; selective bladder preservation with twice-daily radiation plus 5-flourouracil/cisplatin or daily radiation plus gemcitabine for patients with muscle invasive bladder cancer -primary results of NRG/RTOG 0712: a randomized phase 2 multicenter trial; and multi-institutional phase 2 trial of high-dose stereotactic body radiation therapy with temporary hydrogel spacer for low-and intermediate-risk prostate cancer. 3.2.2 Preprocess the corpus object Use the tm_map function to apply a transformation on each element of the corpus object. Alternatively use the tm_parLapply function to do the same in parallel. mycorpus &lt;- tm_map(mycorpus, tolower) mycorpus &lt;- tm_map(mycorpus, removePunctuation, ucp=TRUE) mycorpus &lt;- tm_map(mycorpus, removeNumbers) mycorpus &lt;- tm_map(mycorpus, removeWords, stopwords(&quot;en&quot;)) Now that we have normalized the text, lets look at the first abstract again. mycorpus[[1]]$content ## [1] &quot;background international standard radiotherapy schedule early breast cancer delivers gy fractions gy weeks long history nonstandard regimens delivering lower total dose using fewer larger fractions hypofractionation aimed test benefits radiotherapy schedules using fraction sizes larger gy terms localregional tumour control normal tissue responses quality life economic consequences women prescribed postoperative radiotherapy methods women early breast cancer pta pn m centres uk randomly assigned primary surgery receive gy fractions gy weeks gy fractions gy weeks women eligible trial aged years immediate reconstruction available followup randomisation method computer generated blinded protocolspecified principal endpoints localregional tumour relapse defined reappearance cancer irradiated sites late normal tissue effects quality life analysis intention treat study registered international standard randomised controlled trial number isrctn findings women assigned gy group gy group median follow years iqr rate localregional tumour relapse years ci gy group ci gy group representing absolute difference ci ie absolute difference localregional relapse better worse gy gy photographic patient selfassessments indicated lower rates late adverse effects gy gy interpretation radiation schedule delivering gy fractions seems offer rates localregional tumour relapse late adverse effects least favourable standard schedule gy fractions&quot; It looks ‘normalized’ but how do we model this? how do we apply NLP algorithms on it? 3.3 The Bag of Words Representation Consider: what is a text document to a computer? What can it do with a sequence of characters? In order for us to apply statistical methods on a document, we need a representation of texts that is easy for a computer to process, but still encodes information related to that text’s content. One such representation is the Bag of Words format. Bag of Words is a way of representing a document that encodes a document as a ‘bag’ of its tokens. The document is represented as the words that appeared in the document and the number of times those words appeared. All information about word order is lost in this representation, however, for many NLP methods, this is still an effective representation of the content of the document. bag of words image The power of the bag of words representation is that each document can be represented in the same vector space. We do so by defining the vector dimensions to reflect the vocabulary across all the documents. The vectors can then be merged into a matrix called a Document Term Matrix. dtm image 3.4 The Document Term Matrix In brief, a Document Term Matrix: - each document is represented by a set of tokens and their counts - the order of tokens is not encoded in this representation - the basis of many text processing methods, including document classification and topic modeling In R we can use a DocumentTermMatrix function from the ‘tm’ package to create this structure from our corpus. 3.4.1 Creating a Document Term Matrix from the corpus object From the ‘corpus’ object we can create a document term matrix. mydtm &lt;- DocumentTermMatrix(mycorpus) Note: the DocumentTermMatrix automatically sets all the characters to lower case. 3.4.2 Exploring with a DTM A useful tool is the inspect function from the ‘tm’ package. inspect(mydtm) ## &lt;&lt;DocumentTermMatrix (documents: 714, terms: 8180)&gt;&gt; ## Non-/sparse entries: 85943/5754577 ## Sparsity : 99% ## Maximal term length: 40 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs breast cancer dose patients radiation results therapy treated treatment ## 125 6 3 0 8 0 0 0 0 3 ## 18 6 0 4 4 0 1 1 4 1 ## 192 12 2 0 17 0 1 1 1 4 ## 203 14 29 0 2 6 1 3 4 0 ## 207 10 4 2 9 3 1 0 2 1 ## 272 7 4 7 7 1 2 1 0 1 ## 322 4 1 3 3 1 0 1 2 1 ## 323 6 1 11 4 1 0 1 1 8 ## 598 5 1 3 13 0 1 1 0 6 ## 622 6 6 0 13 1 1 2 0 4 ## Terms ## Docs years ## 125 2 ## 18 0 ## 192 3 ## 203 6 ## 207 5 ## 272 0 ## 322 0 ## 323 0 ## 598 5 ## 622 2 From this format it we can find word counts and document lengths. document_lengths &lt;- rowSums(as.matrix(mydtm)) word_counts &lt;- colSums(as.matrix(mydtm)) We can get a sorted list of the biggest documents. sorted_document_lengths &lt;- sort(document_lengths, decreasing=TRUE) barplot(sorted_document_lengths[1:10], col = &quot;tan&quot;, las = 2) Or a sorted list of words and their frequencies. sorted_word_counts &lt;- sort(word_counts, decreasing=TRUE) barplot(sorted_word_counts[1:10], col = &quot;tan&quot;, las = 2) 3.5 TF-IDF TF-IDF stands for term frequency-inverse document frequency. It is a VERY popular method for finding documents relevant to a users search term. It can also be used as an effective (often times better than simple bag of words) representation of documents for statistical modeling of documents in a corpus. 3.5.1 Intuition behind TF-IDF TF-IDF combines two attributes that may signal a words importance in a document into a single metric. The first is the ‘term frequency (TF)’ - how often the word appeared within that document. It makes intuitive sense that if a word appears many times in a document, that the document is about something related to that term. The second attribute is the ‘inverse document frequency (IDF)’ - a measure of what proportion of the documents the word appeared in. If a word appears in all documents, its weight should be reduced. Conversely, if a word appears only in few documents, it should be highly weighted for those documents. 3.5.2 TF-IDF formula tfidf(t,d,D) = tf(t,d) * idf(t,D) Where, tf(t,d) is a function of a terms(t) frequency for a given document(d). And, idf(t,D) is the inverse function of a terms(t) appearance across all the documents(D). There are many variations of functions for tf(t,d) and idf(t,D) that can be used for computing TF-IDF. 3.5.3 TF-IDF from DTM Create tf-idf weighted DTM in R tfidf_dtm &lt;- weightTfIdf(mydtm, normalize=TRUE) Inspect the new dtm inspect(tfidf_dtm) ## &lt;&lt;DocumentTermMatrix (documents: 714, terms: 8180)&gt;&gt; ## Non-/sparse entries: 85943/5754577 ## Sparsity : 99% ## Maximal term length: 40 ## Weighting : term frequency - inverse document frequency (normalized) (tf-idf) ## Sample : ## Terms ## Docs apbi boost dose grade group local survival toxicity ## 121 0 0 0.000000000 0 0 0.00000000 0.000000000 0.00000000 ## 16 0 0 0.000000000 0 0 0.00000000 0.032447894 0.01528079 ## 177 0 0 0.009260951 0 0 0.00000000 0.006907363 0.00000000 ## 291 0 0 0.000000000 0 0 0.00000000 0.000000000 0.00000000 ## 294 0 0 0.000000000 0 0 0.00000000 0.000000000 0.00000000 ## 307 0 0 0.000000000 0 0 0.01274054 0.000000000 0.00000000 ## 443 0 0 0.000000000 0 0 0.00000000 0.000000000 0.00000000 ## 54 0 0 0.000000000 0 0 0.00000000 0.011921597 0.00000000 ## 593 0 0 0.000000000 0 0 0.00000000 0.000000000 0.00000000 ## 7 0 0 0.000000000 0 0 0.00000000 0.000000000 0.00000000 ## Terms ## Docs women years ## 121 0 0 ## 16 0 0 ## 177 0 0 ## 291 0 0 ## 294 0 0 ## 307 0 0 ## 443 0 0 ## 54 0 0 ## 593 0 0 ## 7 0 0 Compare the tfidf representation with the tf representation for a single abstract tf_doc10 &lt;- as.matrix(mydtm[10,]) tf &lt;- colSums(tf_doc10) tfidf_doc10 &lt;- as.matrix(tfidf_dtm[10,]) tfidf &lt;- colSums(tfidf_doc10) barplot(sort(tf, decreasing=TRUE)[1:10], col = &quot;tan&quot;, las = 2) barplot(sort(tfidf, decreasing=TRUE)[1:10], col = &quot;tan&quot;, las = 2) "],["exploratory-data-analysis.html", "4 Exploratory data analysis 4.1 Principal components analysis", " 4 Exploratory data analysis Having now imported the data as a TF-IDF matrix, we aree able to begin with some exploratory data analysis. The analysis can help us to understand patterns in the big stack of abstracts about breast cancer. It won’t tell us much about any particular one of the abstracts - remember that we’ve reduced each one to a vector of term frequencies so as a text document each abstract has been stripped of meaning. But we are going to be able to identify large-scale patterns in the collection of abstracts much more quickly than if we had to read each one. 4.1 Principal components analysis Principal components analysis, which will be abbreviated PCA, is a method used in exploratory data analysis for multidimensional data. That’s what we’re working with, since the 8180 columns of the TF-IDF matrix are more than we could possibly explore on a column-by-column basis. 4.1.1 Check the TF-IDF matrix Not only are there too many columns to work through individually, but also each column is mostly zeroes. As a result an individual column of the TF-IDF matrix doesn’t contain much information. This makes sense if you consider consider how much you would learn about books published in 2020 by counting the number of times each one used ther word “whale”. Here’s an example. # first, convert the TF-IDF to a dense matrix tfmat = as.matrix( tfidf_dtm ) # identify the first term and plot its histogram hist( tfmat[, 1], main=paste0( &#39;uses of &quot;&#39;, colnames( tfmat )[[1]], &#39;&quot;&#39; )) # identify the second term and plot its histgram hist( tfmat[, 2], main=paste0( &#39;uses of &quot;&#39;, colnames( tfmat )[[2]], &#39;&quot;&#39; )) # plot the first two columns plot( as.matrix(tfidf_dtm[, 1:2]), bty=&#39;n&#39;, main=paste0( &#39;joint uses of &quot;&#39;, colnames( tfmat )[[1]], &#39;&quot; and &quot;&#39;, colnames( tfmat )[[2]], &#39;&quot;&#39;) ) 4.1.2 Compute PCA Since the individual columns don’t convey much information, PCA rotates the big matrix until we are “looking” down the “direction” (or component) with the most variability. The result is another matrix of equal size to the first, but each column contains information from all of the columns of the original matrix, and they are sorted so that the columns with the most variability are first. This will take your computer several seconds to calculate. # rotate the TF-IDF so the columns are articles articles = t( tfmat ) # calculate PCA on the rotated TF-IDF pca = prcomp(articles, center=TRUE, scale=TRUE) # extract the matrix of rotations pcmat = as.data.frame( pca$rotation ) 4.1.3 Plot the abstracts We can now look at a plot of the first two principal components to see how it contains more information than the first columns of the TF-IDF matrix. # plot the first two principal components with( pcmat, plot(PC1, PC2) ) Unlike the TF-IDF, the x and y axes here have no inherent meaning, but they are aligned with the greatest variability in the data. So by inspecting the articles that are at the extremes, we can begin to understand the strongest signals that PCA detected in the articles. 4.1.4 Examine how principal components sort the documents # identify the order of documents along the first principal component indx &lt;- order( pcmat$PC1 ) # view the titles at the extremes of the first principal component data$title[ head(indx) ] ## [1] &quot;Signal transduction pathways regulated by CSF-1 receptors modulate the in vitro radiosensitivity of mammary epithelial cells.&quot; ## [2] &quot;Creation of an episode-based payment model for prostate and breast cancer radiation therapy.&quot; ## [3] &quot;Erratum to: Wang SY, Kelly G, Gross C, et al. Information needs of older women with early-stage breast cancer when making radiation therapy decisions. [Int J Radiat Oncol Biol Phys 2017;98:733-740](S0360301617303097)(10.1016/j.ijrobp.2017.02.001)).&quot; ## [4] &quot;Molecular radiation therapy in experimental model of advanced prostate cancer using internalizable monoclonal antibody.&quot; ## [5] &quot;Dna double-strand break repair and induction of apoptosis in relation to late normal tissue responses following radiation therapy for early breast cancer.&quot; ## [6] &quot;Patient experience survey of early-stage breast cancer patients undergoing whole-breast radiation therapy.&quot; data$title[ tail(indx) ] ## [1] &quot;Phase I/II Study Evaluating Early Tolerance in Breast Cancer Patients Undergoing Accelerated Partial Breast Irradiation Treated With the MammoSite Balloon Breast Brachytherapy Catheter Using a 2-Day Dose Schedule.&quot; ## [2] &quot;Accelerated partial breast irradiation using sole interstitial multicatheter brachytherapy versus whole breast irradiation for early breast cancer: Five-year results of a randomized phase 3 trial-Part I: Local control and survival results.&quot; ## [3] &quot;Accelerated partial-breast irradiation provides equivalent 10-year outcomes to whole breast irradiation: A matched-pair analysis.&quot; ## [4] &quot;Daily Fractionation of External Beam Accelerated Partial Breast Irradiation to 40 Gy Is Well Tolerated and Locally Effective.&quot; ## [5] &quot;Margin status an indication for accelerated partial breast irradiation in early-stage breast cancers.&quot; ## [6] &quot;Differences in Patterns of Failure in Patients Treated With Accelerated Partial Breast Irradiation Versus Whole-Breast Irradiation: A Matched-Pair Analysis With 10-Year Follow-Up.&quot; It seems that the PCA has detected a pattern in term usage that distinguishes between papers on one extreme that make comparisons between whole- and partial-breast radiation therapy, and papers on the other extreme that are mostly about molecular biology. This is kind of impressive when you consider that the computer did it without any human input about what language might be relevant to medicine or biology. Let’s look at how some other principal components are separating the documents. # identify the order of documents along the second principal component indx = order( pcmat$PC2 ) # observe which papers are at the extremes of the second PC data$title[ head(indx) ] ## [1] &quot;The influence of young age on outcome in early stage breast cancer.&quot; ## [2] &quot;Locoregional failure in early-stage breast cancer patients treated with breast-conserving therapy: Which patients benefit from supraclavicular nodal irradiation?&quot; ## [3] &quot;Breast conservation treatment of early stage breast cancer: Patterns of failure.&quot; ## [4] &quot;Equivalent survival with breast conservation therapy or mastectomy in the management of youngwomen with early-stage breast cancer.&quot; ## [5] &quot;Delivering adjuvant radiation therapy beyond 6 months after breast-conserving surgery does not jeopardize the locoregional control in early-stage breast cancer patients.&quot; ## [6] &quot;Profile of prognostic factors in 1022 Indian women with early-stage breast cancer treated with breast-conserving therapy.&quot; data$title[ tail(indx) ] ## [1] &quot;Hypofractionated whole breast radiation therapy: Does size matter?&quot; ## [2] &quot;Comparison of preoperative partial breast radiosurgery treatment techniques: 3D-CRT, non-coplanar IMRT, coplanar IMRT, and VMAT.&quot; ## [3] &quot;A dosimetric planning study for hypofractionated whole breast irradiation with concurrent boost (RTOG 1005) for early stage breast cancer.&quot; ## [4] &quot;Prone hypofractionated whole-breast radiotherapy without a boost to the tumor bed: Comparable toxicity of IMRT versus a 3D conformal technique.&quot; ## [5] &quot;Ongoing clinical experience utilizing 3D conformal external beam radiotherapy to deliver partial-breast irradiation in patients with early-stage breast cancer treated with breast-conserving therapy.&quot; ## [6] &quot;Prone accelerated partial breast irradiation after breast-conserving surgery: Preliminary clinical results and dose-volume histogram analysis.&quot; With the second principal component, we see a separation between papers about breast-conserving therapy at one extreme and papers about hypofractionated, conformal, or modulated radiation therapy at the other extreme. # identify the order of documents along the third principal component indx = order( pcmat$PC3 ) # observe which papers are at the extremes of the second PC data$title[ head(indx) ] ## [1] &quot;External beam radiation therapy versus intraoperative radiation therapy for breast-conserving therapy: A large single-institution matched-pair evaluation.&quot; ## [2] &quot;Early results of a prospective cohort study on intraoperative radiation therapy for early breast cancer at a single institution.&quot; ## [3] &quot;Cosmetic outcome and late breast toxicity after intraoperative radiation therapy as a single modality or as a boost using the intrabeam® device: A prospective study.&quot; ## [4] &quot;Updated results of a prospective cohort study on intraoperative radiation therapy for early breast cancer at a single-institution.&quot; ## [5] &quot;First safety analysis after 80 treated patients with early breast cancer within the targit-e trial.&quot; ## [6] &quot;A comparison of the early toxicities of balloon catheter high-dose-rate brachytherapy (BCHDRB) and intraoperative radiation therapy.&quot; data$title[ tail(indx) ] ## [1] &quot;Dosimetric comparison of radiotherapy for left sided breast cancer: Breath-hold versus free breathing.&quot; ## [2] &quot;Is there justification for the utilization of IMRT in the treatment of early-stage right breast cancer?&quot; ## [3] &quot;Bilateral breast and regional nodal irradiation in early stage breast cancer D a dosimetric comparison of IMRT and 3D conformal radiation therapy.&quot; ## [4] &quot;Small arc volumetric modulated arc therapy: A new approach superior to IMRT in optimizing dosimetric and treatment relevant parameters for patients following breast conservative surgery.&quot; ## [5] &quot;Comparison of different radiation techniques to achieve normal tissue sparing and target volume coverage in the treatment of left-sided early stage breast cancer.&quot; ## [6] &quot;Planning comparison of intensity modulated radiation therapy delivered with 2 tangential fields versus 3-dimensional conformal radiotherapy for cardiac sparing in women with left-sided breast cancer.&quot; At one extreme of the third principal component are articles about intraoperative radiation therapy and at the other extreme are articles about radiation dosimetry, especially with respect to modulated and conformal radiation therapy. 4.1.5 Interpreting PCA It looks like there are patterns of actual meaning arising from the PCA. We’re not talking here about statistical significance or clinical importance. It is still for humans to decide what patterns have clinical importance. Here, PCA is a shortcut that allows us to identify some patterns in articles about breast cancer radiation therapy without having to read all of the articles. All of the patterns I’ve cited are based on patterns of word usage, and yet it appears that those patterns in word usage actually map onto distinctions with real-world meaning. So keep in mind that the principal components are only identifying patterns in term frequencies but the PCA is useful only when those terms map onto real-world meaning. So far, we’ve looked at the most extreme documents along some principal components. If you want to see more than a few documents at a time, or see how documents align jointly on two principal components, then you want to do a scatterplot of the top terms of each document. I’ll show you how, but it uses a function called top_terms() that needs to be loaded from our Github repository (it’s too complex to live-code). # load the function for identifying top terms in documents source(url(&quot;https://ucdavisdatalab.github.io/workshop-nlp-healthcare/top_terms.R&quot;)) # cbind data and the PCA rotation plotdata &lt;- cbind(data, pcmat) # identify top terms for each document and attach them to the plotdata plotdata[[ &#39;top_terms&#39; ]] &lt;- top_terms( tfidf_dtm ) # visualize the top principal components in terms of top terms ggplot(plotdata) + aes(x=PC2, y=PC3, label=top_terms) + geom_text(check_overlap=TRUE) We’ve seen the patterns that align with the first, second, and third principal components. What makes these principal components the first, second, and third? The PCs are ranked by the amount of total variance in the TF-IDF matrix that they explain. So the first principal component is aligned with the strongest signal in term frequency, and the second is aligned with the strongest signal that’s left over after accounting for the first component, and so on. The strength of signal is quantified by the proportion of variance in the TF-IDF matrix that is explained by each principal component. In order to see how the strength of signal changes along the sequence of PCs, we can visualize the cumulative percent of variance explained. plot(100 * cumsum(pca$sdev^2) / sum(pca$sdev^2), type=&#39;l&#39;, bty=&#39;n&#39;, ylab=&quot;% total variance explained&quot;, xlab=&quot;Number of components&quot;) "]]
